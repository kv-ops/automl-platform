# Add this section to pyproject.toml under [project.optional-dependencies]

# GPU Computing & Acceleration
gpu = [
    "cupy-cuda11x>=12.0.0,<13.0.0",
    "pycuda>=2022.2.2",
    "numba[cuda]>=0.58.0",
    "gputil>=1.4.0",
    "nvidia-ml-py3>=7.352.0",
    "pynvml>=11.5.0",
    "gpustat>=1.1.1",
    "onnxruntime-gpu>=1.16.0,<2.0.0",
    "pytorch-memlab>=0.3.0",
    "torch-tb-profiler>=0.4.0",
]

# Deep learning (already exists, update versions)
deep = [
    "torch>=2.1.0,<3.0.0",
    "torchvision>=0.16.0,<1.0.0",
    "torchaudio>=2.1.0,<3.0.0",
    "tensorflow>=2.15.0,<3.0.0",
    "pytorch-tabnet>=4.1.0",
    "pytorch-lightning>=2.1.0",
]

# Advanced distributed GPU training
distributed-gpu = [
    "horovod>=0.28.0,<1.0.0",
    "fairscale>=0.4.0,<1.0.0",
    "deepspeed>=0.12.0,<1.0.0",
]

# AutoML with GPU acceleration
automl-gpu = [
    "autogluon[torch]>=1.0.0",
    "nni>=3.0,<4.0",
]

# GPU inference serving
serving-gpu = [
    "tritonclient[all]>=2.40.0",
    # tensorrt and torch-tensorrt require manual installation
]

# Alternative GPU frameworks
gpu-alt = [
    "jax[cuda11_pip]>=0.4.20",
]

# Complete GPU installation
gpu-complete = [
    "automl-platform[gpu]",
    "automl-platform[deep]",
    "automl-platform[distributed-gpu]",
    "automl-platform[automl-gpu]",
    "automl-platform[serving-gpu]",
]
