# AutoML Platform v3.0 - Enterprise MLOps Edition

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![MLflow](https://img.shields.io/badge/MLflow-2.9%2B-0194E2)](https://mlflow.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104%2B-009688)](https://fastapi.tiangolo.com/)
[![ONNX](https://img.shields.io/badge/ONNX-1.15%2B-5C5C5C)](https://onnx.ai/)
[![River](https://img.shields.io/badge/River-0.19%2B-00CED1)](https://riverml.xyz/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Production-ready AutoML platform with enterprise MLOps capabilities including incremental learning, real-time streaming, advanced scheduling, billing system, and LLM-powered insights.

## ðŸš€ New in v3.0 - Advanced Features

- **Incremental Learning**: Online learning with River/Vowpal Wabbit, drift detection (ADWIN, DDM, EDDM, Page-Hinkley), and replay buffers
- **Real-Time Streaming**: Kafka, Flink, Pulsar, and Redis Streams integration for ML on streaming data
- **Enterprise Scheduling**: DataRobot-inspired job scheduler with GPU/CPU queue separation and plan-based quotas
- **Billing System**: Complete subscription management with usage tracking and Stripe/PayPal integration
- **LLM Integration**: GPT-4/Claude powered data cleaning, feature suggestions, and model explanations
- **Advanced Monitoring**: Prometheus metrics, Slack/Email alerts, PSI calculation, Evidently integration
- **MLflow Registry**: Complete model lifecycle with versioning and stages
- **A/B Testing**: Statistical significance testing (t-test, Mann-Whitney, Chi-square) with Streamlit dashboard
- **Model Export**: ONNX with quantization, PMML, TFLite, CoreML for edge deployment
- **Automated Retraining**: Drift-triggered and schedule-based model retraining

## Key Features

### Core AutoML
- **No Data Leakage**: All preprocessing within CV folds using sklearn Pipeline
- **30+ Algorithms**: Including XGBoost, LightGBM, CatBoost, Neural Networks
- **Hyperparameter Optimization**: Optuna, Grid Search, Random Search
- **Ensemble Methods**: Voting, stacking, blending with meta-learners
- **Imbalance Handling**: SMOTE, class weights, focal loss

### Incremental & Streaming ML
- **Online Learning Models**: 
  - SGD-based incremental models (classification/regression)
  - River models (Hoeffding Trees, Adaptive Random Forest)
  - Neural incremental models with experience replay
- **Drift Detection Algorithms**: ADWIN, DDM, EDDM, Page-Hinkley
- **Stream Processors**: MLStreamProcessor for real-time predictions
- **Replay Buffer**: Configurable buffer size and replay frequency
- **Windowed Aggregation**: Time-based feature aggregation

### Real-Time Streaming
- **Multiple Platforms**: Kafka, Apache Flink, Apache Pulsar, Redis Streams
- **Stream Configuration**: Batch size, window size, checkpoint intervals
- **Exactly-Once Semantics**: Support for guaranteed message processing
- **Streaming Orchestrator**: Unified interface for all streaming platforms
- **Performance Metrics**: Throughput tracking, latency monitoring

### MLOps & Production

#### Model Registry (MLflow)
- **Version Management**: Track all model versions with metadata
- **Stage Transitions**: None â†’ Staging â†’ Production â†’ Archived
- **Model Comparison**: Compare metrics between versions
- **Rollback Support**: Quick rollback to previous versions
- **Signature Inference**: Automatic input/output schema capture

#### A/B Testing Framework
- **Statistical Tests**: t-test, Mann-Whitney U, Chi-square
- **Effect Size Calculation**: Cohen's d, rank biserial, CramÃ©r's V
- **Visualization Suite**: ROC curves, PR curves, confusion matrices, residual plots
- **Traffic Routing**: Configurable traffic splits for champion/challenger
- **Auto-promotion**: Promote winner based on statistical significance

#### Model Export Service
- **ONNX Export**: Dynamic/static quantization, INT8 optimization
- **PMML Support**: For traditional ML deployment
- **Edge Deployment**: Automated package creation with inference scripts
- **Quantization Options**: Dynamic INT8, static INT8 with calibration
- **Multi-format Support**: ONNX, PMML, TFLite (planned), CoreML (planned)

### Enterprise Features
- **Multi-Tenant Architecture**: Secure tenant isolation with resource management
- **Advanced Job Scheduling**:
  - Priority-based queues (GPU_TRAINING, GPU_INFERENCE, CPU_PRIORITY, LLM, BATCH)
  - Plan-based quotas (Free, Trial, Pro, Enterprise)
  - Automatic worker scaling
  - GPU resource management
- **Comprehensive Billing System**:
  - Subscription plans with tiered pricing
  - Usage tracking (API calls, predictions, GPU hours, storage)
  - Payment integration (Stripe, PayPal)
  - Overage charge calculation
- **LLM-Powered Features**:
  - Akkio-style conversational data cleaning
  - Feature engineering suggestions
  - Natural language model explanations
  - RAG-based knowledge queries
  - Code generation for AutoML tasks

### Advanced Monitoring & Alerting
- **Drift Detection**: KS test for numerical, Chi-square for categorical
- **Population Stability Index (PSI)**: Monitor distribution shifts
- **Multi-Channel Alerts**: Slack webhooks, Email (SMTP), generic webhooks
- **Prometheus Metrics**: Complete metrics export for Grafana
- **Data Quality Monitoring**: Schema validation, outlier detection, missing value tracking
- **Billing Metrics**: Cost tracking per tenant/model

## Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Incremental Learning](#incremental-learning)
- [Streaming ML](#streaming-ml)
- [Job Scheduling](#job-scheduling)
- [Billing System](#billing-system)
- [LLM Features](#llm-features)
- [MLOps Workflow](#mlops-workflow)
- [A/B Testing](#ab-testing)
- [Model Export](#model-export)
- [Monitoring & Alerts](#monitoring--alerts)
- [Project Structure](#project-structure)
- [API Documentation](#api-documentation)
- [Configuration](#configuration)
- [UI Dashboards](#ui-dashboards)
- [Testing](#testing)
- [Development](#development)
- [Deployment](#deployment)
- [Performance](#performance)
- [Contributing](#contributing)

## Installation

### Prerequisites

- Python 3.8 or higher
- Redis (for caching and workers)
- MLflow (for model registry)
- Optional: Kafka/Pulsar (for streaming), Docker, Airflow/Prefect
- Optional: PostgreSQL/MySQL (for production)

### Quick Installation

```bash
# Clone the repository
git clone https://github.com/your-org/automl-platform.git
cd automl-platform

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Basic installation (recommended)
pip install -r requirements-minimal.txt

# Or use the installation script
./install_mlops.sh
```

### Installation Options

```bash
# Basic MLOps installation
pip install -r requirements-minimal.txt

# Full installation (includes all features)
pip install -r requirements.txt

# GPU support
pip install -r requirements-gpu.txt

# Streaming support
pip install kafka-python apache-flink pulsar-client redis

# Incremental learning
pip install river vowpalwabbit

# LLM features
pip install openai anthropic langchain

# A/B testing and visualization
pip install scipy matplotlib seaborn

# Model export
pip install onnx onnxruntime skl2onnx sklearn2pmml

# Complete installation with script
./install_mlops.sh --gpu --airflow --streaming --llm
```

## Quick Start

### 1. Start Services

```bash
# Start MLflow server
mlflow server --host 0.0.0.0 --port 5000

# Start API server
uvicorn app:app --host 0.0.0.0 --port 8000 --reload

# Start Streamlit Dashboard (A/B Testing UI)
streamlit run automl_platform/streamlit_ab_testing.py

# Start Redis (for caching and job queue)
redis-server

# Start Kafka (for streaming - optional)
docker-compose up -d kafka zookeeper

# Start Airflow (for workflow orchestration - optional)
airflow standalone
```

### 2. Basic AutoML Training

```python
from automl_platform.orchestrator import AutoMLOrchestrator
from automl_platform.config import AutoMLConfig
import pandas as pd

# Load data
df = pd.read_csv("your_data.csv")
X = df.drop("target", axis=1)
y = df["target"]

# Configure
config = AutoMLConfig()
config.mlflow_tracking_uri = "http://localhost:5000"
config.max_trials = 50
config.time_limit = 3600

# Train with automatic registration
orchestrator = AutoMLOrchestrator(config)
orchestrator.fit(
    X, y,
    task="classification",
    register_best_model=True,
    model_name="customer_churn"
)

# Make predictions
predictions = orchestrator.predict(X_test)

# Export for deployment
orchestrator.export_best_model(
    format="onnx",
    quantize=True,
    output_path="models/churn_model.onnx"
)
```

## Incremental Learning

### Online Learning with River

```python
from automl_platform.incremental_learning import (
    IncrementalConfig, 
    RiverIncrementalModel,
    IncrementalPipeline
)

# Configure incremental learning
config = IncrementalConfig(
    batch_size=32,
    learning_rate=0.01,
    enable_replay=True,
    detect_drift=True,
    drift_detector="adwin",
    checkpoint_frequency=1000
)

# Create River model (Hoeffding Adaptive Tree)
model = RiverIncrementalModel(config, model_type="hoeffding_adaptive_tree")

# Create pipeline
pipeline = IncrementalPipeline(config)
pipeline.set_model(model)

# Process streaming data
def data_generator():
    """Your streaming data source"""
    while True:
        X_batch, y_batch = get_next_batch()  # Your data source
        yield X_batch, y_batch

# Train on stream
stats = pipeline.process_stream(
    data_generator(),
    max_samples=100000
)

# Evaluate with prequential (test-then-train)
scores = pipeline.evaluate_prequential(
    data_generator(),
    metric="accuracy"
)
print(f"Average accuracy: {np.mean(scores):.3f}")
```

### SGD-based Incremental Learning

```python
from automl_platform.incremental_learning import SGDIncrementalModel

# Create SGD incremental model
model = SGDIncrementalModel(config, task="classification")

# Incremental training
for X_batch, y_batch in data_stream:
    model.partial_fit(X_batch, y_batch, classes=np.unique(y))
    
    # Check for drift
    if model.drift_detector and model.drift_detector.drift_detected:
        print("Drift detected! Triggering model adaptation...")
        # Handle drift (e.g., increase learning rate, reset model)
```

### Streaming Ensemble

```python
from automl_platform.incremental_learning import StreamingEnsemble

# Create ensemble of incremental models
ensemble = StreamingEnsemble(
    config,
    base_models=["sgd", "river_tree", "neural"]
)

# Train ensemble
for X_batch, y_batch in data_stream:
    ensemble.partial_fit(X_batch, y_batch)
    predictions = ensemble.predict(X_batch)
```

## Streaming ML

### Kafka Stream Processing

```python
from automl_platform.api.streaming import (
    StreamConfig,
    MLStreamProcessor,
    KafkaStreamHandler,
    StreamingOrchestrator
)

# Configure streaming
config = StreamConfig(
    platform="kafka",
    brokers=["localhost:9092"],
    topic="sensor_data",
    consumer_group="ml_predictions",
    batch_size=100,
    window_size=60,
    checkpoint_interval=30
)

# Create ML stream processor
processor = MLStreamProcessor(config, model=your_trained_model)

# Create Kafka handler
handler = KafkaStreamHandler(config)

# Start consuming and predicting
orchestrator = StreamingOrchestrator(config)
orchestrator.set_processor(processor)

# Start streaming pipeline
import asyncio
asyncio.run(orchestrator.start(output_topic="predictions"))

# Monitor metrics
metrics = orchestrator.get_metrics()
print(f"Throughput: {metrics['throughput_per_sec']} msg/s")
```

### Flink Integration

```python
# Configure for Flink
config = StreamConfig(
    platform="flink",
    brokers=["localhost:9092"],
    topic="transactions",
    enable_exactly_once=True
)

# Create Flink handler
handler = FlinkStreamHandler(config)
handler.create_pipeline(processor)
handler.start()
```

### Windowed Aggregation

```python
from automl_platform.api.streaming import WindowedAggregator

# Create windowed aggregator
aggregator = WindowedAggregator(
    window_size=60,  # 60 seconds
    slide_interval=10  # Slide every 10 seconds
)

# Process stream with aggregation
for message in stream:
    aggregator.add(message.key, message.value, message.timestamp)
    
    # Get aggregates for current window
    aggregates = aggregator.get_aggregates(message.key, message.timestamp)
    print(f"Window stats: {aggregates}")
```

## Job Scheduling

### Submit Jobs with Priority Queues

```python
from automl_platform.scheduler import (
    SchedulerFactory,
    JobRequest,
    QueueType,
    PlanType
)
from automl_platform.api.billing import BillingManager

# Initialize scheduler with billing
billing_manager = BillingManager()
scheduler = SchedulerFactory.create_scheduler(config, billing_manager)

# Create job request
job = JobRequest(
    tenant_id="tenant_123",
    user_id="user_456",
    plan_type=PlanType.PRO.value,
    task_type="train",
    queue_type=QueueType.GPU_TRAINING,  # GPU queue for Pro users
    payload={
        "dataset_id": "data_001",
        "model_type": "xgboost",
        "task": "classification"
    },
    estimated_memory_gb=8.0,
    estimated_time_minutes=45,
    requires_gpu=True,
    num_gpus=1
)

# Submit job
job_id = scheduler.submit_job(job)
print(f"Job submitted: {job_id}")

# Check status
status = scheduler.get_job_status(job_id)
print(f"Status: {status.status.value}")

# Get queue statistics
stats = scheduler.get_queue_stats()
print(f"Active jobs: {stats['active_jobs']}")
print(f"GPU workers: {stats['gpu_workers']}")
```

### Plan-based Quotas

```python
# Plans automatically enforce limits:
# - Free: 1 concurrent job, no GPU
# - Trial: 2 concurrent jobs, 4 workers (DataRobot-style)
# - Pro: 5 concurrent jobs, 10 GPU hours/month
# - Enterprise: Unlimited

# Check if user can submit job
if not scheduler._check_quotas(job):
    print("Quota exceeded! Upgrade plan for more resources")
```

## Billing System

### Subscription Management

```python
from automl_platform.api.billing import BillingManager, PlanType, BillingPeriod

# Initialize billing manager
billing = BillingManager()

# Create subscription
subscription = billing.create_subscription(
    tenant_id="company_123",
    plan_type=PlanType.PROFESSIONAL,
    billing_period=BillingPeriod.MONTHLY,
    payment_method="stripe"
)

# Track usage
billing.usage_tracker.track_api_call("company_123", "/predict")
billing.usage_tracker.track_predictions("company_123", count=1000)
billing.usage_tracker.track_gpu_usage("company_123", hours=2.5)

# Calculate current bill
bill = billing.calculate_bill("company_123")
print(f"Current charges: ${bill['total']:.2f}")
print(f"Overage charges: ${bill['overage_charges']:.2f}")

# Process payment
payment_result = billing.process_payment(
    "company_123",
    amount=bill['total'],
    payment_method="stripe"
)
```

### Usage Limits by Plan

| Feature | Free | Starter | Professional | Enterprise |
|---------|------|---------|--------------|------------|
| Models | 3 | 10 | 50 | Unlimited |
| Predictions/month | 1,000 | 10,000 | 100,000 | Unlimited |
| API calls/day | 100 | 1,000 | 10,000 | Unlimited |
| GPU hours/month | 0 | 0 | 10 | 100 |
| Storage (GB) | 1 | 10 | 100 | 1,000 |
| Team members | 1 | 3 | 10 | Unlimited |
| Price/month | $0 | $49 | $299 | $999 |

## LLM Features

### Conversational Data Cleaning (Akkio-style)

```python
# Via API
POST /api/v1/llm/clean/chat
{
    "dataset_id": "sales_data",
    "message": "Remove outliers in price column and fill missing values with median",
    "apply_changes": true
}

# Via WebSocket for interactive session
ws://localhost:8000/api/v1/llm/clean/interactive?dataset_id=sales_data
```

### Feature Engineering Suggestions

```python
# Request feature suggestions
POST /api/v1/llm/features/suggest
{
    "dataset_id": "customer_data",
    "target_column": "churn",
    "max_suggestions": 10,
    "include_code": true
}

# Response
{
    "suggestions": [
        {
            "name": "days_since_last_purchase",
            "formula": "(today - last_purchase_date).days",
            "rationale": "Customer recency is highly predictive of churn",
            "expected_impact": "high",
            "code": "df['days_since_last_purchase'] = ..."
        }
    ]
}
```

### Model Explanations

```python
# Generate natural language explanation
POST /api/v1/llm/explain/model
{
    "model_id": "model_123",
    "audience": "business",  # or "technical", "executive"
    "include_shap": true
}

# Response
{
    "explanation": "The customer churn model primarily relies on three factors: 
                   1) Days since last purchase (35% importance)
                   2) Customer lifetime value (28% importance)
                   3) Support ticket count (22% importance)
                   
                   The model achieves 92% accuracy by identifying patterns...",
    "shap_interpretation": {...}
}
```

### RAG-based Knowledge Queries

```python
# Query knowledge base
POST /api/v1/llm/rag/query
{
    "query": "What are best practices for handling imbalanced datasets?",
    "context_type": "documentation",
    "max_results": 5
}

# Index custom documents
POST /api/v1/llm/rag/index
{
    "documents": ["doc1", "doc2"],
    "collection_name": "ml_best_practices"
}
```

## MLOps Workflow

### Complete MLOps Pipeline with MLflow

```python
from automl_platform.mlflow_registry import MLflowRegistry
from automl_platform.ab_testing import ABTestingService
from automl_platform.export_service import ModelExporter

# Initialize components
config = AutoMLConfig()
config.mlflow_tracking_uri = "http://localhost:5000"
registry = MLflowRegistry(config)
ab_service = ABTestingService(registry)
exporter = ModelExporter()

# 1. Train and register model
orchestrator = AutoMLOrchestrator(config)
orchestrator.fit(X_train, y_train)

# Register with MLflow
model_version = registry.register_model(
    model=orchestrator.best_model,
    model_name="customer_churn",
    metrics={"accuracy": 0.92, "f1": 0.89},
    params={"max_depth": 10, "n_estimators": 100},
    X_sample=X_train[:100],
    y_sample=y_train[:100],
    description="XGBoost model for customer churn prediction",
    tags={"team": "data_science", "project": "retention"}
)

# 2. Promote to staging
registry.promote_model("customer_churn", version=1, stage="Staging")

# 3. A/B test against production
test_id = ab_service.create_ab_test(
    model_name="customer_churn",
    champion_version=1,
    challenger_version=2,
    traffic_split=0.1,
    min_samples=1000,
    confidence_level=0.95,
    primary_metric="accuracy"
)

# 4. Monitor and analyze
results = ab_service.get_test_results(test_id)
if results['p_value'] < 0.05 and results['winner'] == 'challenger':
    registry.promote_model("customer_churn", version=2, stage="Production")
    ab_service.conclude_test(test_id, promote_winner=True)

# 5. Export for deployment
export_result = exporter.export_to_onnx(
    model=orchestrator.best_model,
    sample_input=X_train[:10],
    model_name="customer_churn",
    quantize=True
)
print(f"Model exported to: {export_result['path']}")
print(f"Size reduction: {export_result['size_reduction']}")
```

## A/B Testing

### Statistical Testing Framework

```python
from automl_platform.ab_testing import (
    ABTestingService,
    MetricsComparator,
    StatisticalTester
)

# Initialize A/B testing service
ab_service = ABTestingService(registry)

# Create A/B test
test_id = ab_service.create_ab_test(
    model_name="fraud_detection",
    champion_version=3,
    challenger_version=4,
    traffic_split=0.2,  # 20% to challenger
    min_samples=500,
    confidence_level=0.95,
    primary_metric="precision",
    statistical_test="mann_whitney"  # or "t_test", "chi_square"
)

# Route predictions
for request in incoming_requests:
    model_type, version = ab_service.route_prediction(test_id)
    
    # Make prediction with selected model
    model = registry.load_model("fraud_detection", version=version)
    prediction = model.predict(request.features)
    
    # Record result
    ab_service.record_result(
        test_id=test_id,
        model_type=model_type,
        success=True,
        metric_value=prediction.confidence,
        response_time=prediction.latency
    )

# Get results with statistical analysis
results = ab_service.get_test_results(test_id)
print(f"P-value: {results['p_value']:.4f}")
print(f"Effect size: {results['effect_size']:.3f}")
print(f"Winner: {results['winner']}")
print(f"Confidence: {results['confidence']:.2%}")

# Conclude test
ab_service.conclude_test(test_id, promote_winner=True)
```

### Offline Model Comparison

```python
# Compare models offline with visualization
comparison = ab_service.compare_models_offline(
    model_a=model_v1,
    model_b=model_v2,
    X_test=X_test,
    y_test=y_test,
    task="classification"
)

# Results include:
# - Metrics comparison (accuracy, precision, recall, F1, ROC-AUC)
# - Statistical significance tests
# - Visualizations (ROC curves, PR curves, confusion matrices)
print(f"Model B improvement: {comparison['comparison']['accuracy_improvement_pct']:.1f}%")

# Access visualizations (base64 encoded)
roc_plot = comparison['visualizations']['roc_curves']
confusion_matrices = comparison['visualizations']['confusion_matrices']
```

### Sample Size Calculation

```python
# Calculate required sample size for desired statistical power
from automl_platform.ab_testing import StatisticalTester

required_samples = StatisticalTester.calculate_sample_size(
    effect_size=0.2,  # Small effect
    alpha=0.05,       # Significance level
    power=0.80        # Statistical power
)
print(f"Required samples per group: {required_samples}")
```

## Model Export

### ONNX Export with Quantization

```python
from automl_platform.export_service import ModelExporter, ExportConfig

# Configure export
config = ExportConfig(
    output_dir="./exported_models",
    quantize=True,
    optimize_for_edge=True,
    target_opset=13
)

exporter = ModelExporter(config)

# Export to ONNX
result = exporter.export_to_onnx(
    model=trained_model,
    sample_input=X_sample,
    model_name="customer_model",
    dynamic_axes={'input': {0: 'batch_size'}}  # Dynamic batch
)

print(f"Original size: {result['size_mb']} MB")
print(f"Quantized size: {result['quantized_size_mb']} MB")
print(f"Size reduction: {result['size_reduction']}")
print(f"Inference test: {result['inference_test']['inference_time_ms']} ms")
```

### Edge Deployment Package

```python
# Create complete edge deployment package
edge_result = exporter.export_for_edge(
    model=trained_model,
    sample_input=X_sample,
    model_name="edge_model",
    formats=['onnx', 'tflite', 'coreml']
)

# Package includes:
# - Optimized models (quantized ONNX, TFLite, CoreML)
# - inference.py script for edge deployment
# - requirements_edge.txt
# - README with usage instructions
# - Dockerfile for containerized deployment
# - Benchmark utilities

print(f"Package created at: {edge_result['package_dir']}")
print(f"Best format: {edge_result['exports']['onnx_quantized']['best_option']}")
```

### PMML Export

```python
# Export to PMML for traditional deployment
pmml_result = exporter.export_to_pmml(
    pipeline=sklearn_pipeline,
    sample_input=X_train,
    sample_output=y_train,
    model_name="traditional_model"
)

print(f"PMML model saved to: {pmml_result['path']}")
```

## Monitoring & Alerts

### Configure Multi-Channel Alerts

```python
from automl_platform.monitoring import (
    MonitoringService,
    ModelMonitor,
    AlertManager,
    DataQualityMonitor
)

# Initialize monitoring service
monitoring = MonitoringService(billing_tracker=billing_manager)

# Register model for monitoring
monitor = monitoring.register_model(
    model_id="model_123",
    model_type="classification",
    reference_data=X_train,
    tenant_id="company_123"
)

# Configure alerts
alert_config = {
    "accuracy_threshold": 0.85,
    "drift_threshold": 0.3,
    "latency_threshold": 1.0,
    "quality_score_threshold": 80,
    "billing_threshold": 1000.0,
    "notification_channels": ["slack", "email", "log"]
}

# Set up Slack webhook
os.environ['SLACK_WEBHOOK_URL'] = "https://hooks.slack.com/services/..."
os.environ['ALERT_EMAIL_RECIPIENTS'] = "team@company.com"

alert_manager = AlertManager(alert_config)

# Monitor predictions
for batch in prediction_batches:
    monitor.log_prediction(
        features=batch['features'],
        predictions=batch['predictions'],
        actuals=batch['actuals'],
        prediction_time=batch['latency']
    )
    
    # Check for alerts
    metrics = monitor.get_performance_summary()
    alerts = alert_manager.check_alerts(metrics)
```

### Data Quality Monitoring

```python
# Initialize quality monitor
quality_monitor = DataQualityMonitor(expected_schema={
    'age': 'int64',
    'income': 'float64',
    'category': 'object'
})

# Check data quality
quality_report = quality_monitor.check_data_quality(
    new_data_batch,
    tenant_id="company_123"
)

print(f"Quality Score: {quality_report['quality_score']}")
print(f"Issues: {quality_report['issues']}")
print(f"Warnings: {quality_report['warnings']}")
```

### Prometheus Metrics

```python
# Access metrics at http://localhost:8000/metrics
GET /metrics

# Available metrics:
# - ml_predictions_total
# - ml_model_accuracy
# - ml_data_drift_score
# - ml_prediction_duration_seconds
# - ml_billing_api_calls_total
# - ml_billing_compute_seconds_total
```

## Project Structure

```
automl-platform/
â”œâ”€â”€ app.py                           # Main FastAPI application
â”œâ”€â”€ config.yaml                      # Configuration file
â”œâ”€â”€ requirements.txt                 # Full dependencies
â”œâ”€â”€ requirements-minimal.txt        # Minimal dependencies (recommended)
â”œâ”€â”€ requirements-gpu.txt            # GPU dependencies
â”œâ”€â”€ install_mlops.sh               # Installation script
â”œâ”€â”€ .gitignore                      # Git ignore file
â”œâ”€â”€ DEPLOYMENT_GUIDE.md            # Deployment documentation
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ automl_platform/               # Main package
â”‚   â”œâ”€â”€ orchestrator.py           # AutoML orchestrator (enhanced)
â”‚   â”œâ”€â”€ mlflow_registry.py        # MLflow model registry integration
â”‚   â”œâ”€â”€ ab_testing.py              # A/B testing with statistical analysis
â”‚   â”œâ”€â”€ export_service.py         # Model export (ONNX/PMML/Edge)
â”‚   â”œâ”€â”€ retraining_service.py     # Automated retraining
â”‚   â”œâ”€â”€ incremental_learning.py   # Online learning module
â”‚   â”œâ”€â”€ data_prep.py              # Data preprocessing
â”‚   â”œâ”€â”€ model_selection.py        # Model selection & HPO
â”‚   â”œâ”€â”€ monitoring.py             # Advanced monitoring with alerts
â”‚   â”œâ”€â”€ scheduler.py              # Job scheduling with quotas
â”‚   â”œâ”€â”€ streamlit_ab_testing.py   # Streamlit UI for A/B tests
â”‚   â”œâ”€â”€ api/                      # API components
â”‚   â”‚   â”œâ”€â”€ mlops_endpoints.py    # MLOps REST endpoints
â”‚   â”‚   â”œâ”€â”€ billing.py            # Billing & subscriptions
â”‚   â”‚   â”œâ”€â”€ billing_middleware.py # Billing middleware
â”‚   â”‚   â”œâ”€â”€ llm_endpoints.py      # LLM-powered endpoints
â”‚   â”‚   â””â”€â”€ streaming.py          # Streaming ML endpoints
â”‚   â””â”€â”€ examples/                  # Examples
â”‚       â””â”€â”€ mlops_integration.py  # Complete MLOps workflow
â””â”€â”€ tests/                         # Test suite (TO BE IMPLEMENTED)
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_mlflow_registry.py   # (Planned)
    â”œâ”€â”€ test_ab_testing.py         # (Planned)
    â”œâ”€â”€ test_export_service.py     # (Planned)
    â”œâ”€â”€ test_incremental.py        # (Planned)
    â”œâ”€â”€ test_streaming.py          # (Planned)
    â”œâ”€â”€ test_scheduler.py          # (Planned)
    â”œâ”€â”€ test_billing.py            # (Planned)
    â”œâ”€â”€ test_monitoring.py         # (Planned)
    â””â”€â”€ integration/               # (Planned)
        â””â”€â”€ test_mlops_workflow.py # (Planned)
```

## API Documentation

### MLOps Endpoints

Access interactive API docs at `http://localhost:8000/docs`

#### Model Registry

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/mlops/models/register` | Register new model with MLflow |
| POST | `/api/v1/mlops/models/promote` | Promote model stage |
| GET | `/api/v1/mlops/models/{name}/versions` | Get version history |
| POST | `/api/v1/mlops/models/{name}/rollback` | Rollback to previous version |
| GET | `/api/v1/mlops/models/{name}/compare` | Compare two versions |
| DELETE | `/api/v1/mlops/models/{name}/{version}` | Delete model version |

#### A/B Testing

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/mlops/ab-tests/create` | Create A/B test |
| GET | `/api/v1/mlops/ab-tests/active` | List active tests |
| POST | `/api/v1/mlops/ab-tests/{id}/route` | Route to champion/challenger |
| POST | `/api/v1/mlops/ab-tests/{id}/record` | Record prediction result |
| GET | `/api/v1/mlops/ab-tests/{id}/results` | Get test results with stats |
| POST | `/api/v1/mlops/ab-tests/{id}/conclude` | Conclude and promote |
| POST | `/api/v1/mlops/ab-tests/{id}/pause` | Pause test |
| POST | `/api/v1/mlops/ab-tests/{id}/resume` | Resume test |

#### Model Export

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/mlops/models/export/onnx` | Export to ONNX |
| POST | `/api/v1/mlops/models/export/pmml` | Export to PMML |
| POST | `/api/v1/mlops/models/export/edge` | Create edge package |
| GET | `/api/v1/mlops/models/export/{id}/download` | Download exported model |
| POST | `/api/v1/mlops/models/quantize` | Quantize ONNX model |

#### LLM Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/llm/clean/chat` | Conversational data cleaning |
| WebSocket | `/api/v1/llm/clean/interactive` | Interactive cleaning session |
| POST | `/api/v1/llm/features/suggest` | Get feature suggestions |
| POST | `/api/v1/llm/features/auto-engineer` | Auto-engineer features |
| POST | `/api/v1/llm/explain/model` | Generate model explanation |
| POST | `/api/v1/llm/explain/prediction` | Explain individual prediction |
| POST | `/api/v1/llm/reports/generate` | Generate comprehensive report |
| POST | `/api/v1/llm/rag/query` | Query knowledge base |
| POST | `/api/v1/llm/code/generate` | Generate AutoML code |

#### Streaming Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/streaming/start` | Start stream processing |
| GET | `/api/v1/streaming/status/{id}` | Get stream status |
| POST | `/api/v1/streaming/stop/{id}` | Stop stream processing |
| GET | `/api/v1/streaming/metrics/{id}` | Get streaming metrics |

#### Billing Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/billing/subscription/create` | Create subscription |
| PUT | `/api/v1/billing/subscription/upgrade` | Upgrade plan |
| GET | `/api/v1/billing/usage` | Get usage statistics |
| GET | `/api/v1/billing/invoice` | Get current invoice |

#### Job Scheduling

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/jobs/submit` | Submit job to queue |
| GET | `/api/v1/jobs/{id}/status` | Get job status |
| DELETE | `/api/v1/jobs/{id}` | Cancel job |
| GET | `/api/v1/jobs/queue/stats` | Queue statistics |

## Configuration

### Complete Configuration Example

```yaml
# Environment
environment: production
debug: false

# API Configuration
api:
  host: 0.0.0.0
  port: 8000
  enable_auth: true
  enable_rate_limit: true
  cors_origins: ["http://localhost:3000"]

# MLflow Configuration
mlflow_tracking_uri: "http://localhost:5000"
mlflow_experiment_name: "automl_experiments"
model_registry_uri: "http://localhost:5000"
enable_ab_testing: true

# A/B Testing Configuration
ab_testing:
  default_confidence_level: 0.95
  min_samples_per_variant: 100
  max_test_duration_days: 30
  auto_promote_winner: false
  statistical_tests: ["t_test", "mann_whitney", "chi_square"]

# Model Export Configuration
export:
  output_dir: "./exported_models"
  enable_onnx: true
  enable_pmml: true
  enable_quantization: true
  optimize_for_edge: true
  target_opset: 13
  quantization_types: ["dynamic_int8", "static_int8"]

# Streaming Configuration
streaming:
  enabled: true
  platforms:
    kafka:
      brokers: ["localhost:9092"]
      topics: ["predictions", "training_data"]
    flink:
      job_manager: "localhost:8081"
  checkpoint_interval: 30
  window_size: 60

# Incremental Learning
incremental:
  enabled: true
  drift_detection: true
  drift_detectors: ["adwin", "ddm", "page_hinkley"]
  replay_buffer_size: 10000
  checkpoint_frequency: 1000

# Job Scheduling
scheduling:
  backend: "celery"  # or "ray"
  broker_url: "redis://localhost:6379/0"
  result_backend: "redis://localhost:6379/0"
  autoscale_enabled: true
  autoscale_min_workers: 2
  autoscale_max_workers: 10
  gpu_workers: 2

# Billing Configuration
billing:
  enabled: true
  stripe_secret_key: ${STRIPE_SECRET_KEY}
  paypal_client_id: ${PAYPAL_CLIENT_ID}
  default_plan: "free"
  trial_days: 14

# LLM Configuration
llm:
  enabled: true
  providers:
    openai:
      api_key: ${OPENAI_API_KEY}
      model: "gpt-4"
    anthropic:
      api_key: ${ANTHROPIC_API_KEY}
      model: "claude-3-opus-20240229"
  enable_rag: true
  vector_store: "chromadb"

# Monitoring & Alerts
monitoring:
  enabled: true
  drift_detection_enabled: true
  prometheus_enabled: true
  prometheus_port: 8001
  alerting:
    enabled: true
    channels:
      slack:
        webhook_url: ${SLACK_WEBHOOK_URL}
      email:
        smtp_host: "smtp.gmail.com"
        smtp_port: 587
        from_email: "alerts@automl.com"
    thresholds:
      accuracy_min: 0.85
      drift_max: 0.3
      latency_max_ms: 1000
      quality_score_min: 80

# Storage Configuration
storage:
  backend: "minio"  # or "s3", "gcs", "azure"
  endpoint: "localhost:9000"
  access_key: ${STORAGE_ACCESS_KEY}
  secret_key: ${STORAGE_SECRET_KEY}
  models_bucket: "models"
  datasets_bucket: "datasets"
  checkpoints_bucket: "checkpoints"

# Database Configuration
database:
  url: "postgresql://user:pass@localhost/automl"
  pool_size: 10
  max_overflow: 20

# Worker Configuration
worker:
  max_workers: 8
  task_time_limit: 3600
  task_soft_time_limit: 3300
  worker_prefetch_multiplier: 2
```

## UI Dashboards

### Streamlit A/B Testing Dashboard

```bash
# Start the dashboard
streamlit run automl_platform/streamlit_ab_testing.py

# Access at http://localhost:8501
```

Features:
- **Active Tests View**: Monitor all running A/B tests with progress bars
- **Test Creation**: Configure statistical tests, traffic splits, and success metrics
- **Model Comparison**: Side-by-side comparison with statistical significance
- **Results Visualization**: ROC curves, PR curves, confusion matrices
- **Test Analytics**: Historical performance and winner analysis

### Integration with Main App

```python
# In your main Streamlit app
from automl_platform.streamlit_ab_testing import integrate_ab_testing_to_main_app

# Add to navigation
if menu_selection == "A/B Testing":
    integrate_ab_testing_to_main_app(ab_service, registry)
```

## Testing

### Test Suite

The platform includes comprehensive unit tests for core MLOps components. The test suite covers model registry integration, A/B testing framework, and model export functionality.

```bash
# Test structure
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_mlflow_registry.py       # MLflow integration tests
â”œâ”€â”€ test_ab_testing.py            # A/B testing framework tests
â”œâ”€â”€ test_export_service.py        # Model export tests
â”œâ”€â”€ test_incremental.py           # (Planned) Incremental learning tests
â”œâ”€â”€ test_streaming.py             # (Planned) Streaming component tests
â”œâ”€â”€ test_scheduler.py             # (Planned) Job scheduling tests
â”œâ”€â”€ test_billing.py               # (Planned) Billing system tests
â”œâ”€â”€ test_monitoring.py            # (Planned) Monitoring tests
â””â”€â”€ integration/                  # (Planned) Integration tests
    â”œâ”€â”€ test_mlops_workflow.py
    â”œâ”€â”€ test_streaming_pipeline.py
    â””â”€â”€ test_ab_testing_flow.py
```

### Test Coverage

#### MLflow Registry Tests (`test_mlflow_registry.py`)
- âœ… Model registration with metadata and signature inference
- âœ… Version management and stage promotion (None â†’ Staging â†’ Production â†’ Archived)
- âœ… Model comparison between versions with metrics diff calculation
- âœ… Model rollback functionality with automatic archiving
- âœ… Model loading by version or stage
- âœ… Model version deletion
- âœ… Model search with filters
- âœ… Model history retrieval with run details

#### A/B Testing Tests (`test_ab_testing.py`)
- âœ… A/B test creation and configuration
- âœ… Traffic routing with configurable splits
- âœ… Statistical testing (t-test, Mann-Whitney U, Chi-square)
- âœ… Effect size calculations (Cohen's d, rank biserial, CramÃ©r's V)
- âœ… Sample size calculation for desired power
- âœ… Automatic winner determination based on p-value and minimum improvement
- âœ… Test lifecycle management (active, paused, concluded)
- âœ… Offline model comparison with visualization generation
- âœ… Classification metrics comparison (accuracy, precision, recall, F1, ROC-AUC)
- âœ… Regression metrics comparison (MSE, MAE, RMSE, RÂ²)

#### Export Service Tests (`test_export_service.py`)
- âœ… ONNX export with dynamic/fixed batch size
- âœ… ONNX model validation and inference testing
- âœ… Dynamic quantization (INT8) with size reduction tracking
- âœ… Static quantization with calibration data
- âœ… PMML export for traditional deployment
- âœ… Edge deployment package creation
- âœ… Automatic script generation for edge inference
- âœ… Dockerfile and requirements generation
- âœ… Multi-format export (ONNX, PMML, TFLite, CoreML)

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-cov pytest-mock unittest

# Run all available tests
pytest tests/ -v --cov=automl_platform

# Run specific test modules
pytest tests/test_mlflow_registry.py -v
pytest tests/test_ab_testing.py -v
pytest tests/test_export_service.py -v

# Run with coverage report
pytest tests/ --cov=automl_platform --cov-report=html --cov-report=term

# Run specific test class
pytest tests/test_ab_testing.py::TestStatisticalTester -v

# Run specific test method
pytest tests/test_mlflow_registry.py::TestMLflowRegistry::test_register_model -v

# Run tests with verbose output
pytest tests/ -vv

# Run tests in parallel (requires pytest-xdist)
pip install pytest-xdist
pytest tests/ -n auto
```

### Test Execution Examples

#### Running MLflow Registry Tests
```bash
# Test MLflow integration
python -m pytest tests/test_mlflow_registry.py -v

# Expected output:
# tests/test_mlflow_registry.py::TestMLflowRegistry::test_initialization PASSED
# tests/test_mlflow_registry.py::TestMLflowRegistry::test_register_model PASSED
# tests/test_mlflow_registry.py::TestMLflowRegistry::test_promote_model PASSED
# tests/test_mlflow_registry.py::TestMLflowRegistry::test_get_model_history PASSED
# tests/test_mlflow_registry.py::TestMLflowRegistry::test_compare_models PASSED
# tests/test_mlflow_registry.py::TestMLflowRegistry::test_rollback_model PASSED
# ... [11 tests total]
```

#### Running A/B Testing Tests
```bash
# Test A/B testing framework
python -m pytest tests/test_ab_testing.py -v

# Expected output:
# tests/test_ab_testing.py::TestABTestConfig::test_config_creation PASSED
# tests/test_ab_testing.py::TestStatisticalTester::test_t_test PASSED
# tests/test_ab_testing.py::TestStatisticalTester::test_mann_whitney PASSED
# tests/test_ab_testing.py::TestStatisticalTester::test_chi_square PASSED
# tests/test_ab_testing.py::TestABTestingService::test_create_ab_test PASSED
# tests/test_ab_testing.py::TestABTestingService::test_route_prediction PASSED
# ... [20+ tests total]
```

#### Running Export Service Tests
```bash
# Test model export functionality
python -m pytest tests/test_export_service.py -v

# Expected output:
# tests/test_export_service.py::TestExportConfig::test_default_config PASSED
# tests/test_export_service.py::TestModelExporter::test_export_to_onnx_success PASSED
# tests/test_export_service.py::TestModelExporter::test_quantize_onnx PASSED
# tests/test_export_service.py::TestModelExporter::test_export_for_edge PASSED
# tests/test_export_service.py::TestModelExporter::test_create_edge_deployment_package PASSED
# ... [15+ tests total]
```

### Writing New Tests

#### Test Structure Template
```python
import unittest
from unittest.mock import Mock, patch
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from automl_platform.your_module import YourClass

class TestYourClass(unittest.TestCase):
    def setUp(self):
        """Set up test fixtures."""
        self.instance = YourClass()
    
    def tearDown(self):
        """Clean up after tests."""
        pass
    
    def test_functionality(self):
        """Test specific functionality."""
        result = self.instance.method()
        self.assertEqual(result, expected_value)

if __name__ == "__main__":
    unittest.main()
```

### Test Coverage Report

Current test coverage for implemented modules:

| Module | Coverage | Tests | Status |
|--------|----------|-------|--------|
| mlflow_registry.py | ~85% | 11 | âœ… Implemented |
| ab_testing.py | ~80% | 22 | âœ… Implemented |
| export_service.py | ~75% | 16 | âœ… Implemented |
| incremental_learning.py | 0% | 0 | ðŸ“‹ Planned |
| streaming.py | 0% | 0 | ðŸ“‹ Planned |
| scheduler.py | 0% | 0 | ðŸ“‹ Planned |
| billing.py | 0% | 0 | ðŸ“‹ Planned |
| monitoring.py | 0% | 0 | ðŸ“‹ Planned |

### Continuous Integration

Add this GitHub Actions workflow for automated testing:

```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10]
    
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-minimal.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      run: |
        pytest tests/ -v --cov=automl_platform --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v2
```

### Contributing Tests

To contribute new tests:

1. **Follow the existing test structure** - Use unittest framework and follow naming conventions
2. **Use mocks for external dependencies** - Mock MLflow, ONNX, and other external libraries
3. **Test both success and failure cases** - Include edge cases and error handling
4. **Add docstrings** - Document what each test is verifying
5. **Maintain isolation** - Each test should be independent and not rely on others

Priority areas for new test contributions:
- **Incremental Learning**: Test online learning algorithms and drift detection
- **Streaming**: Test Kafka/Flink/Pulsar handlers and message processing
- **Scheduler**: Test job queue management and priority handling
- **Billing**: Test subscription management and usage tracking
- **Monitoring**: Test alert triggering and metric calculations
- **Integration Tests**: End-to-end workflow testing

## Development

### Setting up Development Environment

```bash
# Install dev dependencies
pip install pytest pytest-cov black flake8 mypy pre-commit

# Setup pre-commit hooks
pre-commit install

# Format code
black automl_platform/

# Type checking
mypy automl_platform/

# Linting
flake8 automl_platform/
```

### Code Quality Tools

```bash
# Run black formatter
black automl_platform/ --line-length 100

# Run flake8 linter
flake8 automl_platform/ --max-line-length 100

# Run mypy type checker
mypy automl_platform/ --ignore-missing-imports

# Run bandit security checker
bandit -r automl_platform/
```

## Deployment

### Docker Deployment

```bash
# Build image
docker build -t automl-platform:latest .

# Run with docker-compose
docker-compose up -d

# Scale workers
docker-compose scale worker=4 gpu-worker=2

# View logs
docker-compose logs -f api worker
```

### Docker Compose Configuration

```yaml
version: '3.8'

services:
  api:
    image: automl-platform:latest
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - redis
      - mlflow
      - postgres

  worker:
    image: automl-platform:latest
    command: celery -A automl_platform.scheduler worker -Q cpu_default,cpu_priority
    scale: 4
    depends_on:
      - redis

  gpu-worker:
    image: automl-platform:latest
    command: celery -A automl_platform.scheduler worker -Q gpu_training,gpu_inference
    runtime: nvidia
    scale: 2
    depends_on:
      - redis

  mlflow:
    image: ghcr.io/mlflow/mlflow
    ports:
      - "5000:5000"
    command: mlflow server --host 0.0.0.0

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:14
    environment:
      POSTGRES_DB: automl
      POSTGRES_USER: automl
      POSTGRES_PASSWORD: automl123
    volumes:
      - postgres_data:/var/lib/postgresql/data

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

volumes:
  postgres_data:
```

### Kubernetes Deployment

```bash
# Deploy with Helm
helm install automl-platform ./helm-chart \
  --set image.tag=latest \
  --set replicas.api=3 \
  --set replicas.worker=6 \
  --set replicas.gpuWorker=2

# Configure autoscaling
kubectl autoscale deployment automl-api \
  --min=2 --max=10 --cpu-percent=70

kubectl autoscale deployment automl-worker \
  --min=4 --max=20 --cpu-percent=80

# Setup ingress
kubectl apply -f k8s/ingress.yaml
```

### Cloud Deployment

#### AWS

```bash
# ECS deployment
ecs-cli compose --file docker-compose.yml up

# Lambda deployment for serverless inference
sam build && sam deploy

# SageMaker endpoint
python deploy/sagemaker_deploy.py
```

#### Google Cloud

```bash
# Cloud Run deployment
gcloud run deploy automl-platform \
  --image gcr.io/project/automl:latest \
  --platform managed \
  --allow-unauthenticated

# GKE with GPU
gcloud container clusters create automl-cluster \
  --accelerator type=nvidia-tesla-t4,count=2 \
  --machine-type n1-standard-4
```

#### Azure

```bash
# Container Instances
az container create \
  --resource-group automl \
  --name automl-platform \
  --image automl:latest \
  --cpu 4 --memory 16

# AKS deployment
az aks create --resource-group automl \
  --name automl-cluster \
  --node-count 3 \
  --enable-addons monitoring
```

## Performance

### Benchmarks

| Operation | Time | Notes |
|-----------|------|-------|
| Training (1M rows, 10 models) | ~30 min | With HPO |
| Incremental update (1K samples) | <500ms | River models |
| Stream processing (10K msg/s) | <100ms latency | Kafka |
| Prediction batch (1K) | <100ms | With preprocessing |
| ONNX Export | <5s | With quantization |
| ONNX Quantization | <10s | 75% size reduction |
| Model Registration | <1s | MLflow backend |
| A/B Test Analysis | <500ms | Statistical tests |
| Drift Detection | <200ms | KS/Chi-square |
| Edge Inference | <10ms | Quantized ONNX |

### Optimization Tips

1. **GPU Acceleration**:
   - Use GPU for XGBoost/LightGBM: 3-5x speedup
   - Enable mixed precision for neural networks
   - Batch predictions for GPU utilization

2. **Model Export Optimization**:
   - Use dynamic quantization for 75% size reduction
   - Enable ONNX optimization for edge devices
   - Use static quantization with calibration for best accuracy/size tradeoff

3. **A/B Testing Optimization**:
   - Cache statistical test results
   - Use approximate tests for large samples
   - Batch result recording for high-traffic scenarios

4. **Streaming Optimization**:
   - Increase batch size for throughput
   - Use exactly-once semantics sparingly
   - Enable compression for Kafka

5. **Incremental Learning**:
   - Tune replay buffer size vs memory
   - Use appropriate drift detector for data type
   - Checkpoint frequently for large streams

6. **Resource Management**:
   - Use priority queues for critical jobs
   - Enable autoscaling for workers
   - Monitor GPU memory usage

7. **Cost Optimization**:
   - Use spot instances for batch jobs
   - Implement proper caching
   - Compress models with quantization

## Troubleshooting

### Common Issues

1. **Kafka Connection Issues**:
   ```bash
   # Check Kafka is running
   docker-compose ps kafka
   
   # Test connection
   kafka-topics --list --bootstrap-server localhost:9092
   ```

2. **GPU Not Detected**:
   ```bash
   # Check NVIDIA drivers
   nvidia-smi
   
   # Verify CUDA
   python -c "import torch; print(torch.cuda.is_available())"
   ```

3. **MLflow Connection**:
   ```bash
   # Check MLflow is running
   curl http://localhost:5000/health
   
   # Set tracking URI
   export MLFLOW_TRACKING_URI=http://localhost:5000
   ```

4. **ONNX Export Issues**:
   ```python
   # Check ONNX installation
   import onnx
   import onnxruntime
   print(f"ONNX version: {onnx.__version__}")
   print(f"ONNX Runtime version: {onnxruntime.__version__}")
   ```

5. **Worker Queue Issues**:
   ```bash
   # Check Celery workers
   celery -A automl_platform.scheduler inspect active
   
   # Purge queue
   celery -A automl_platform.scheduler purge
   ```

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Process

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

### Code Standards

- Follow PEP 8
- Add type hints for all functions
- Write comprehensive docstrings
- Include unit tests for new features (when test framework is ready)
- Update documentation

### Priority Areas for Contribution

- **Testing**: Help implement the test suite for existing modules
- Additional streaming platform integrations (RabbitMQ, AWS Kinesis)
- More incremental learning algorithms
- Enhanced LLM features
- Additional payment providers
- Performance optimizations
- Documentation improvements

## Changelog

### Version 3.0.0 (2024-01)
- Added comprehensive A/B testing framework with statistical analysis
- Implemented MLflow model registry integration
- Created model export service with ONNX quantization
- Added incremental learning module with River integration
- Implemented streaming ML with Kafka/Flink/Pulsar support
- Created enterprise job scheduler with GPU management
- Added comprehensive billing system
- Integrated LLM-powered features
- Enhanced monitoring with multi-channel alerts
- Added Streamlit A/B testing dashboard

### Version 2.0.0 (2023)
- Initial MLflow integration
- Basic A/B testing framework
- Model export capabilities

### Version 1.0.0 (2023)
- Initial release
- Core AutoML functionality

## Support

- **Documentation**: [Full Documentation](https://automl-platform.readthedocs.io)
- **Issues**: [GitHub Issues](https://github.com/your-org/automl-platform/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/automl-platform/discussions)
- **Slack Community**: [Join our Slack](https://automl-community.slack.com)
- **Email**: support@automl-platform.com

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- MLflow for model registry
- River for incremental learning
- Apache Kafka/Flink for streaming
- Optuna for hyperparameter optimization
- ONNX for model interoperability
- FastAPI for REST API framework
- Streamlit for dashboards
- Celery for distributed task processing
- SciPy for statistical testing
- Matplotlib/Seaborn for visualizations

## Citation

If you use this platform in your research, please cite:

```bibtex
@software{automl_platform,
  title = {AutoML Platform: Enterprise MLOps Edition},
  author = {Your Organization},
  year = {2024},
  version = {3.0.0},
  url = {https://github.com/your-org/automl-platform}
}
```

---

**Built for enterprise ML workflows with production-ready MLOps capabilities**

*Version 3.0 - Last updated: January 2024*
